{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXb/5pN9sacTZa+WtBn6Le",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruvtre/Lossfunk_Code/blob/main/Arxiv_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install feedparser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgG6xuhkmVTd",
        "outputId": "0f3e036a-e705-431c-cde6-670525451311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=9ebfd83635ec1c845681719e84e5227de5f0a1a4d26ecf16449a7ac611e9fe29\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser\n",
            "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datetime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxpPW4nHRbEI",
        "outputId": "4793d2a9-40ff-4cf4-f57f-13410a1acdb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datetime\n",
            "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting zope.interface (from datetime)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datetime) (2025.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface->datetime) (75.2.0)\n",
            "Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-5.5 zope.interface-7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "0BJi42LeRin2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making Search Query Function"
      ],
      "metadata": {
        "id": "n0bdD09LoqGm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oG59ymJfkCj"
      },
      "outputs": [],
      "source": [
        "# Cell A: Imports and helper function\n",
        "import time\n",
        "import requests\n",
        "import feedparser\n",
        "from typing import Literal\n",
        "from urllib.parse import quote\n",
        "\n",
        "def make_search_query(keywords: str | None, title: str | None, categories: list[str] | None, operator: str) -> str:\n",
        "    \"\"\"Build arXiv search query from keywords, title, and categories.\"\"\"\n",
        "    query_parts = []\n",
        "\n",
        "    # Process keywords (search all fields)\n",
        "    if keywords:\n",
        "        # Split by whitespace and escape quotes\n",
        "        terms = keywords.replace('\"', '').split()\n",
        "        if terms:\n",
        "            keyword_query = f\" {operator} \".join(f\"all:{term}\" for term in terms)\n",
        "            query_parts.append(f\"({keyword_query})\")\n",
        "\n",
        "    # Process title\n",
        "    if title:\n",
        "        # Split by whitespace and escape quotes\n",
        "        terms = title.replace('\"', '').split()\n",
        "        if terms:\n",
        "            title_query = f\" {operator} \".join(f\"ti:{term}\" for term in terms)\n",
        "            query_parts.append(f\"({title_query})\")\n",
        "\n",
        "    # Process categories (always OR between categories)\n",
        "    if categories:\n",
        "        cat_query = \" OR \".join(f\"cat:{cat}\" for cat in categories)\n",
        "        query_parts.append(f\"({cat_query})\")\n",
        "\n",
        "    # Join all parts with AND\n",
        "    return \" AND \".join(query_parts) if query_parts else \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Raw API Call Function"
      ],
      "metadata": {
        "id": "mPmFs3vVotO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_arxiv_call(\n",
        "    search_query: str,\n",
        "    start: int = 0,\n",
        "    max_results: int = 100,\n",
        "    sort_by: str = \"relevance\",\n",
        "    sort_order: str = \"descending\",\n",
        "    retries: int = 3,\n",
        "    backoff: float = 3.0,\n",
        ") -> str:\n",
        "    \"\"\"Make HTTP call to arXiv API with retries and rate limiting.\n",
        "\n",
        "    Returns the raw XML feed text.\n",
        "    \"\"\"\n",
        "    # Build URL\n",
        "    base_url = \"http://export.arxiv.org/api/query\"\n",
        "    params = {\n",
        "        \"search_query\": search_query,\n",
        "        \"start\": start,\n",
        "        \"max_results\": max_results,\n",
        "        \"sortBy\": sort_by,\n",
        "        \"sortOrder\": sort_order\n",
        "    }\n",
        "\n",
        "    # URL encode the parameters\n",
        "    param_str = \"&\".join(f\"{k}={quote(str(v))}\" for k, v in params.items())\n",
        "    url = f\"{base_url}?{param_str}\"\n",
        "\n",
        "    print(f\"Query URL: {url}\")\n",
        "\n",
        "    # Set headers with descriptive User-Agent\n",
        "    headers = {\n",
        "        \"User-Agent\": \"arxiv-query-tool/1.0 (Python; research use)\"\n",
        "    }\n",
        "\n",
        "    last_error = None\n",
        "\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            print(f\"Attempt {attempt}/{retries}...\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            response = requests.get(url, headers=headers, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"Fetched in {elapsed:.2f}s\")\n",
        "\n",
        "            # Success - sleep to respect rate limit\n",
        "            if attempt < retries:  # Don't sleep on last attempt\n",
        "                print(f\"Sleeping {backoff}s to respect rate limits...\")\n",
        "                time.sleep(backoff)\n",
        "\n",
        "            return response.text\n",
        "\n",
        "        except Exception as e:\n",
        "            last_error = e\n",
        "            print(f\"Error on attempt {attempt}: {e}\")\n",
        "\n",
        "            # Sleep before retry\n",
        "            if attempt < retries:\n",
        "                print(f\"Retrying in {backoff}s...\")\n",
        "                time.sleep(backoff)\n",
        "\n",
        "    # All retries failed\n",
        "    raise Exception(f\"All {retries} attempts failed. Last error: {last_error}\")"
      ],
      "metadata": {
        "id": "zjpAUov0oS7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parsing Raw ARXIV Feed"
      ],
      "metadata": {
        "id": "0nqs31rdowOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_arxiv_feed(feed_text: str) -> list[dict]:\n",
        "    \"\"\"Parse arXiv feed XML and extract paper metadata.\"\"\"\n",
        "    # Parse feed\n",
        "    feed = feedparser.parse(feed_text)\n",
        "\n",
        "    if hasattr(feed, 'bozo_exception') and feed.bozo_exception:\n",
        "        raise Exception(f\"Feed parsing error: {feed.bozo_exception}\")\n",
        "\n",
        "    # Extract papers\n",
        "    papers = []\n",
        "    # missing_doi_count = 0\n",
        "\n",
        "    for entry in feed.entries:\n",
        "        # Extract version from id\n",
        "        arxiv_id = entry.id.split('/')[-1]  # Get ID from URL\n",
        "        version = \"v1\"  # default\n",
        "        if 'v' in arxiv_id:\n",
        "            parts = arxiv_id.split('v')\n",
        "            if len(parts) == 2 and parts[1].isdigit():\n",
        "                version = f\"v{parts[1]}\"\n",
        "\n",
        "        # # Extract DOI if available\n",
        "        # doi = None\n",
        "        # if hasattr(entry, 'arxiv_doi'):\n",
        "        #     doi = entry.arxiv_doi\n",
        "        # elif 'links' in entry:\n",
        "        #     for link in entry.links:\n",
        "        #         if link.get('title') == 'doi':\n",
        "        #             doi = link.get('href', '').replace('http://dx.doi.org/', '')\n",
        "        #             break\n",
        "\n",
        "        # if not doi:\n",
        "        #     missing_doi_count += 1\n",
        "\n",
        "        # Build paper dict\n",
        "\n",
        "        # Format dates to be more readable\n",
        "        def format_date(date_str):\n",
        "            \"\"\"Convert ISO format to readable format\"\"\"\n",
        "            try:\n",
        "                dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
        "                return dt.strftime('%Y-%m-%d %H:%M:%S UTC')\n",
        "            except:\n",
        "                return date_str\n",
        "        updated_date = format_date(entry.updated)\n",
        "        published_date = format_date(entry.published)\n",
        "\n",
        "\n",
        "        paper = {\n",
        "            \"id\": arxiv_id,\n",
        "            \"version\": version,\n",
        "            \"title\": entry.title.replace('\\n', ' ').strip(),\n",
        "            \"summary\": entry.summary.replace('\\n', ' ').strip(),\n",
        "            \"authors\": [author.name for author in entry.authors],\n",
        "            \"categories\": [tag.term for tag in entry.tags],\n",
        "            \"published\": published_date,\n",
        "            \"updated\": updated_date,\n",
        "            # \"doi\": doi,\n",
        "            \"pdf_url\": f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
        "        }\n",
        "        papers.append(paper)\n",
        "\n",
        "    print(f\"Parsed {len(papers)} papers\")\n",
        "    # if missing_doi_count > 0:\n",
        "    #     print(f\"Warning: {missing_doi_count} papers missing DOI\")\n",
        "\n",
        "    return papers"
      ],
      "metadata": {
        "id": "lRa47erUm_SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wrapper Function"
      ],
      "metadata": {
        "id": "et-MH_M1o00Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def arxiv_query(\n",
        "    *,\n",
        "    keywords: str | None = None,\n",
        "    title: str | None = None,\n",
        "    categories: list[str] | None = None,\n",
        "    operator: Literal[\"AND\", \"OR\"] = \"AND\",\n",
        "    start: int = 0,\n",
        "    max_results: int = 100,\n",
        "    sort_by: Literal[\"relevance\", \"submittedDate\", \"lastUpdatedDate\"] = \"relevance\",\n",
        "    sort_order: Literal[\"ascending\", \"descending\"] = \"descending\",\n",
        "    retries: int = 3,\n",
        "    backoff: float = 3.0,\n",
        ") -> list[dict]:\n",
        "    \"\"\"Query arXiv API and return a list of paper metadata dictionaries.\n",
        "\n",
        "    Args:\n",
        "        keywords: Search terms for all fields\n",
        "        title: Search terms for title only\n",
        "        categories: List of arXiv categories (e.g., [\"cs.AI\", \"cs.LG\"])\n",
        "        operator: How to combine search terms within keywords/title\n",
        "        start: Starting index for results\n",
        "        max_results: Maximum number of results to return\n",
        "        sort_by: Sort criterion for results\n",
        "        sort_order: Sort direction\n",
        "        retries: Number of retry attempts on failure\n",
        "        backoff: Seconds to wait between requests\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries containing paper metadata\n",
        "    \"\"\"\n",
        "    # Step 1: Build search query\n",
        "    search_query = make_search_query(keywords, title, categories, operator)\n",
        "    if not search_query:\n",
        "        print(\"Warning: No search criteria provided\")\n",
        "        return []\n",
        "\n",
        "    # Step 2: Make API call\n",
        "    try:\n",
        "        raw_feed = make_arxiv_call(\n",
        "            search_query=search_query,\n",
        "            start=start,\n",
        "            max_results=max_results,\n",
        "            sort_by=sort_by,\n",
        "            sort_order=sort_order,\n",
        "            retries=retries,\n",
        "            backoff=backoff\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch data from arXiv: {e}\")\n",
        "        return []\n",
        "\n",
        "    # Step 3: Parse results\n",
        "    try:\n",
        "        papers = parse_arxiv_feed(raw_feed)\n",
        "        return papers\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to parse arXiv feed: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "2zzt6aStooZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tests"
      ],
      "metadata": {
        "id": "VkKjF1Ogo2Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying with simple keywords only - return (10)\n",
        "print(\"=== Test 1: Keywords only (relevance) ===\")\n",
        "results1a = arxiv_query(\n",
        "    keywords=\"transformer attention mechanism\",\n",
        "    max_results=5,\n",
        "    sort_by=\"relevance\",\n",
        "    sort_order=\"descending\"\n",
        ")\n",
        "print(f\"Found {len(results1a)} papers\\n\")\n",
        "\n",
        "# Check the first paper in detail\n",
        "if results1a:\n",
        "    paper = results1a[0]\n",
        "    print(\"First paper details:\")\n",
        "    for key, value in paper.items():\n",
        "        print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aV_qbwAo27F",
        "outputId": "d2693f42-415c-4706-dc7f-6ecd28fcb1a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Test 1: Keywords only (relevance) ===\n",
            "Query URL: http://export.arxiv.org/api/query?search_query=%28all%3Atransformer%20AND%20all%3Aattention%20AND%20all%3Amechanism%29&start=0&max_results=10&sortBy=relevance&sortOrder=descending\n",
            "Attempt 1/3...\n",
            "Fetched in 11.51s\n",
            "Sleeping 3.0s to respect rate limits...\n",
            "Parsed 10 papers\n",
            "Warning: 10 papers missing DOI\n",
            "Found 10 papers\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test 2: Simple title only (relevance)\n",
        "print(\"=== Test 2: Title only (relevance) ===\")\n",
        "results2a = arxiv_query(\n",
        "    title=\"attention is all you need\",\n",
        "    max_results=1,\n",
        "    sort_by=\"relevance\",\n",
        "    sort_order=\"descending\"\n",
        ")\n",
        "print(f\"Found {len(results2a)} papers\\n\")\n",
        "\n",
        "# Check the first paper in detail\n",
        "if results2a:\n",
        "    paper = results2a[0]\n",
        "    print(\"First paper details:\")\n",
        "    for key, value in paper.items():\n",
        "        print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56a6M6UhQYXO",
        "outputId": "43f66663-2a08-42c3-e44e-35c3681435e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Test 2: Title only (relevance) ===\n",
            "Query URL: http://export.arxiv.org/api/query?search_query=%28ti%3Aattention%20AND%20ti%3Ais%20AND%20ti%3Aall%20AND%20ti%3Ayou%20AND%20ti%3Aneed%29&start=0&max_results=1&sortBy=relevance&sortOrder=descending\n",
            "Attempt 1/3...\n",
            "Fetched in 5.33s\n",
            "Sleeping 3.0s to respect rate limits...\n",
            "Parsed 1 papers\n",
            "Found 1 papers\n",
            "\n",
            "First paper details:\n",
            "  id: 1706.03762v7\n",
            "  version: v7\n",
            "  title: Attention Is All You Need\n",
            "  summary: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
            "  authors: ['Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Lukasz Kaiser', 'Illia Polosukhin']\n",
            "  categories: ['cs.CL', 'cs.LG']\n",
            "  published: 2017-06-12 17:57:34 UTC\n",
            "  updated: 2023-08-02 00:41:18 UTC\n",
            "  pdf_url: https://arxiv.org/pdf/1706.03762v7.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference for Arxiv Categories - #TODO: Make a dictionary https://arxiv.org/category_taxonomy"
      ],
      "metadata": {
        "id": "P3R_0hZkTw1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying with category and title - return (1)\n",
        "# Test 3: Category and title\n",
        "print(\"=== Test 3: Category + Title ===\")\n",
        "results3 = arxiv_query(\n",
        "    title=\"attention is all you need\",\n",
        "    categories=[\"cs.AI\", \"cs.LG\"],\n",
        "    max_results=1,\n",
        "    sort_by=\"relevance\",\n",
        "    sort_order=\"descending\"\n",
        ")\n",
        "print(f\"Found {len(results3)} papers\")\n",
        "if results3:\n",
        "    print(\"\\nFirst paper details:\")\n",
        "    for key, value in results3[0].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf6t8yHAQPIL",
        "outputId": "f7e010f2-38e4-4edc-ae3c-51d354fe3891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Test 3: Category + Title ===\n",
            "Query URL: http://export.arxiv.org/api/query?search_query=%28ti%3Aattention%20AND%20ti%3Ais%20AND%20ti%3Aall%20AND%20ti%3Ayou%20AND%20ti%3Aneed%29%20AND%20%28cat%3Acs.AI%20OR%20cat%3Acs.LG%29&start=0&max_results=1&sortBy=relevance&sortOrder=descending\n",
            "Attempt 1/3...\n",
            "Fetched in 9.53s\n",
            "Sleeping 3.0s to respect rate limits...\n",
            "Parsed 1 papers\n",
            "Found 1 papers\n",
            "\n",
            "First paper details:\n",
            "  id: 1706.03762v7\n",
            "  version: v7\n",
            "  title: Attention Is All You Need\n",
            "  summary: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
            "  authors: ['Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar', 'Jakob Uszkoreit', 'Llion Jones', 'Aidan N. Gomez', 'Lukasz Kaiser', 'Illia Polosukhin']\n",
            "  categories: ['cs.CL', 'cs.LG']\n",
            "  published: 2017-06-12 17:57:34 UTC\n",
            "  updated: 2023-08-02 00:41:18 UTC\n",
            "  pdf_url: https://arxiv.org/pdf/1706.03762v7.pdf\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying with category and simple keywords - return (10)\n",
        "# Test 4: Category and keywords\n",
        "print(\"=== Test 4: Category + Keywords ===\")\n",
        "results4 = arxiv_query(\n",
        "    keywords=\"diffusion models\",\n",
        "    categories=[\"cs.CV\", \"cs.LG\"],\n",
        "    max_results=1,\n",
        "    sort_by=\"lastUpdatedDate\",\n",
        "    sort_order=\"descending\"\n",
        ")\n",
        "print(f\"Found {len(results4)} papers\")\n",
        "if results4:\n",
        "    print(\"\\nFirst paper details:\")\n",
        "    for key, value in results4[0].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W03_pstfSQnE",
        "outputId": "5b6d0db1-5ee4-40cc-daa7-589bec3a5f94"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Test 4: Category + Keywords ===\n",
            "Query URL: http://export.arxiv.org/api/query?search_query=%28all%3Adiffusion%20AND%20all%3Amodels%29%20AND%20%28cat%3Acs.CV%20OR%20cat%3Acs.LG%29&start=0&max_results=1&sortBy=lastUpdatedDate&sortOrder=descending\n",
            "Attempt 1/3...\n",
            "Fetched in 7.94s\n",
            "Sleeping 3.0s to respect rate limits...\n",
            "Parsed 1 papers\n",
            "Found 1 papers\n",
            "\n",
            "First paper details:\n",
            "  id: 2506.05350v1\n",
            "  version: v1\n",
            "  title: Contrastive Flow Matching\n",
            "  summary: Unconditional flow-matching trains diffusion models to transport samples from a source distribution to a target distribution by enforcing that the flows between sample pairs are unique. However, in conditional settings (e.g., class-conditioned models), this uniqueness is no longer guaranteed--flows from different conditions may overlap, leading to more ambiguous generations. We introduce Contrastive Flow Matching, an extension to the flow matching objective that explicitly enforces uniqueness across all conditional flows, enhancing condition separation. Our approach adds a contrastive objective that maximizes dissimilarities between predicted flows from arbitrary sample pairs. We validate Contrastive Flow Matching by conducting extensive experiments across varying model architectures on both class-conditioned (ImageNet-1k) and text-to-image (CC3M) benchmarks. Notably, we find that training models with Contrastive Flow Matching (1) improves training speed by a factor of up to 9x, (2) requires up to 5x fewer de-noising steps and (3) lowers FID by up to 8.9 compared to training the same models with flow matching. We release our code at: https://github.com/gstoica27/DeltaFM.git.\n",
            "  authors: ['George Stoica', 'Vivek Ramanujan', 'Xiang Fan', 'Ali Farhadi', 'Ranjay Krishna', 'Judy Hoffman']\n",
            "  categories: ['cs.CV']\n",
            "  published: 2025-06-05 17:59:58 UTC\n",
            "  updated: 2025-06-05 17:59:58 UTC\n",
            "  pdf_url: https://arxiv.org/pdf/2506.05350v1.pdf\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying with category, keywords, title - return (10)\n",
        "# Test 5: Category, keywords, and title\n",
        "print(\"=== Test 5: Category + Keywords + Title ===\")\n",
        "results5 = arxiv_query(\n",
        "    keywords=\"attention mechanism\",\n",
        "    title=\"transformer\",\n",
        "    categories=[\"cs.CL\", \"cs.LG\"],\n",
        "    max_results=1,\n",
        "    sort_by=\"lastUpdatedDate\",\n",
        "    sort_order=\"descending\"\n",
        ")\n",
        "print(f\"Found {len(results5)} papers\")\n",
        "if results5:\n",
        "    print(\"\\nFirst paper details:\")\n",
        "    for key, value in results5[0].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "hB053TorSR5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de92298e-7e6a-4939-bdad-5fa1f236e4db"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Test 5: Category + Keywords + Title ===\n",
            "Query URL: http://export.arxiv.org/api/query?search_query=%28all%3Aattention%20AND%20all%3Amechanism%29%20AND%20%28ti%3Atransformer%29%20AND%20%28cat%3Acs.CL%20OR%20cat%3Acs.LG%29&start=0&max_results=1&sortBy=lastUpdatedDate&sortOrder=descending\n",
            "Attempt 1/3...\n",
            "Fetched in 2.68s\n",
            "Sleeping 3.0s to respect rate limits...\n",
            "Parsed 1 papers\n",
            "Found 1 papers\n",
            "\n",
            "First paper details:\n",
            "  id: 2506.05249v1\n",
            "  version: v1\n",
            "  title: On the Convergence of Gradient Descent on Learning Transformers with   Residual Connections\n",
            "  summary: Transformer models have emerged as fundamental tools across various scientific and engineering disciplines, owing to their outstanding performance in diverse applications. Despite this empirical success, the theoretical foundations of Transformers remain relatively underdeveloped, particularly in understanding their training dynamics. Existing research predominantly examines isolated components--such as self-attention mechanisms and feedforward networks--without thoroughly investigating the interdependencies between these components, especially when residual connections are present. In this paper, we aim to bridge this gap by analyzing the convergence behavior of a structurally complete yet single-layer Transformer, comprising self-attention, a feedforward network, and residual connections. We demonstrate that, under appropriate initialization, gradient descent exhibits a linear convergence rate, where the convergence speed is determined by the minimum and maximum singular values of the output matrix from the attention layer. Moreover, our analysis reveals that residual connections serve to ameliorate the ill-conditioning of this output matrix, an issue stemming from the low-rank structure imposed by the softmax operation, thereby promoting enhanced optimization stability. We also extend our theoretical findings to a multi-layer Transformer architecture, confirming the linear convergence rate of gradient descent under suitable initialization. Empirical results corroborate our theoretical insights, illustrating the beneficial role of residual connections in promoting convergence stability.\n",
            "  authors: ['Zhen Qin', 'Jinxin Zhou', 'Zhihui Zhu']\n",
            "  categories: ['cs.LG', 'math.OC']\n",
            "  published: 2025-06-05 17:10:22 UTC\n",
            "  updated: 2025-06-05 17:10:22 UTC\n",
            "  pdf_url: https://arxiv.org/pdf/2506.05249v1.pdf\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}